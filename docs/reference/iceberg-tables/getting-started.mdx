---
title: Getting started with Apache Iceberg
sidebar_label: Getting Started
description: Starting Apache Spark as a compute engine, with Apache Iceberg provider with additional optimization and packaging
last_update:
  date: 09/30/2022
  author: Vugar Dadalov
---

**IOMETE** uses [Apache Spark](https://spark.apache.org/) as a compute engine, with [Apache Iceberg](https://iceberg.apache.org/) provider with additional optimization and packaging.

---

## Apache Iceberg

Apache Iceberg is an open table format for huge analytic datasets. It provides a rich feature set:

- ACID transactions and insert/merge/delete
- [Schema evolution](https://iceberg.apache.org/evolution#schema-evolution) supports add, drop, update, or rename, and has [no side-effects](https://iceberg.apache.org/evolution#correctness)
- [Hidden partitioning](https://iceberg.apache.org/partitioning) prevents user mistakes that cause silently incorrect results or extremely slow queries
- [Partition layout evolution](https://iceberg.apache.org/evolution#partition-evolution) can update the layout of a table as data volume or query patterns change
- [Time travel](https://iceberg.apache.org/spark-queries#time-travelr) enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes
- Version rollback allows users to quickly correct problems by resetting tables to a good state

:::info
Iceberg is the default provider. That means the followings have the same effect:

- `create table test1(id string);`
- `create table test1(id string) using icerberg;`
  :::

<br />

### Creating a table

```sql
CREATE TABLE table1 (id bigint, data string);
```

Iceberg supports the full range of SQL DDL commands, including:

- [CREATE TABLE ... PARTITIONED BY](https://iceberg.apache.org/spark-ddl/#create-table)
- [CREATE TABLE ... AS SELECT](https://iceberg.apache.org/spark-ddl/#create-table-as-select)
- [ALTER TABLE](https://iceberg.apache.org/spark-ddl/#alter-table)
- [DROP TABLE](https://iceberg.apache.org/spark-ddl/#drop-table)

<br />

### Writing

Once your table is created, insert data using [INSERT INTO:](https://iceberg.apache.org/spark-writes/#insert-into)

```sql
INSERT INTO table1 VALUES (1, 'a'), (2, 'b'), (3, 'c');
INSERT INTO table1 SELECT id, data FROM source WHERE length(data) = 1;
```

Row-level SQL updates using [MERGE INTO](https://iceberg.apache.org/spark-writes/#merge-into) and [DELETE FROM](https://iceberg.apache.org/docs/latest/spark-writes/):

```sql
MERGE INTO local.db.target t USING (SELECT * FROM updates) u ON t.id = u.id
WHEN MATCHED THEN UPDATE SET t.count = t.count + u.count
WHEN NOT MATCHED THEN INSERT *
```

<br />

### Reading

```sql
SELECT count(1) as count, data
FROM table1
GROUP BY data
```

To view all of the snapshots in a table, use the `snapshots` metadata table:

```sql
SELECT * FROM default.table1.snapshots
```

| committed_at            | snapshot_id    | parent_id | operation | manifest_list                                      |
| ----------------------- | -------------- | --------- | --------- | -------------------------------------------------- |
| 2019-02-08 03:29:51.215 | 57897183625154 | null      | append    | s3://.../table/metadata/snap-57897183625154-1.avro |
| ...                     | ...            | ...       | ...       | ...                                                |
